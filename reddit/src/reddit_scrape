import praw
import pandas as pd
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from datetime import datetime
import os
from dotenv import load_dotenv

# --- LOAD ENV ---
load_dotenv()

# --- CONFIG ---
REDDIT_CLIENT_ID = os.getenv("REDDIT_CLIENT_ID")
REDDIT_SECRET = os.getenv("REDDIT_SECRET")
REDDIT_USER = os.getenv("REDDIT_USER")
REDDIT_PASS = os.getenv("REDDIT_PASS")
USER_AGENT = os.getenv("USER_AGENT")

SUBREDDITS = ['bonds']  
POST_LIMIT = 5000  # Fetch more than needed, then filter

CSV_PATH = 'reddit/csvs/reddit_sentiment_results.csv'

# --- INIT ---
analyzer = SentimentIntensityAnalyzer()
reddit = praw.Reddit(
    client_id=REDDIT_CLIENT_ID,
    client_secret=REDDIT_SECRET,
    username=REDDIT_USER,
    password=REDDIT_PASS,
    user_agent=USER_AGENT,
    check_for_async=False
)

# --- LOAD EXISTING DATA ---
if os.path.exists(CSV_PATH):
    existing_df = pd.read_csv(CSV_PATH, parse_dates=["created"])
    latest_time = existing_df["created"].max()
    print(f"üîÅ Last saved post was at: {latest_time}")
else:
    existing_df = pd.DataFrame()
    latest_time = datetime.min
    print("üÜï No existing data found. Scraping everything.")

# --- FETCH NEW POSTS ---
all_posts = []

for sub in SUBREDDITS:
    subreddit = reddit.subreddit(sub)
    print(f"üîç Checking r/{sub}...")

    for post in subreddit.new(limit=POST_LIMIT):
        post_time = datetime.fromtimestamp(post.created_utc)

        if post_time <= latest_time:
            continue  # skip older or duplicate posts

        score = analyzer.polarity_scores(post.title)['compound']
        label = (
            "positive" if score >= 0.05
            else "negative" if score <= -0.05
            else "neutral"
        )

        all_posts.append({
            "subreddit": sub,
            "title": post.title,
            "created": post_time,
            "url": post.url,
            "sentiment_score": score,
            "sentiment_label": label
        })

# --- SAVE ---
if not all_posts:
    print("‚úÖ No new posts to add.")
else:
    new_df = pd.DataFrame(all_posts)
    combined = pd.concat([existing_df, new_df], ignore_index=True).drop_duplicates(subset=["url"])
    combined.to_csv(CSV_PATH, index=False)
    print(f"‚úÖ Added {len(new_df)} new posts. Total now: {len(combined)}")
